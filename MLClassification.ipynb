{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import PunktSentenceTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy \n",
    "import sklearn\n",
    "import random\n",
    "from pprint import pprint\n",
    "import string\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    \n",
    "    data = pd.read_csv(\"BookReviews.csv\", encoding = 'latin1')\n",
    "    \n",
    "    #without specifying \"encoding\" paramter, threw a \"Unicode Decode Error\"\n",
    "    #help found at:\n",
    "    #https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python\n",
    "    \n",
    "    \n",
    "    data = data.sample(frac = 1).reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    #df.sample() returns a random sample of the data in df. The kwarg frac specifies the fraction of the total df\n",
    "    #     that you wish to sample. Then frac = 1 samples the entire df.\n",
    "    #Then, df.reset_index() resets the index of the df, and the kwarg \"drop = True\" \n",
    "    #     prevents the old index being stored as a column in the df\n",
    "    #****help found at:\n",
    "    #https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows\n",
    "    \n",
    "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]  #had several (about 4) columns titles \"Unnamed\"\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPOS(df, ps):\n",
    "    \n",
    "    taggedTxt = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        \n",
    "        txt = str(df.iloc[i])  #I had problems where this was a float, which doesn't make much sense\n",
    "        txt = txt.lower()\n",
    "        tggd = tag_txt(txt, ps)\n",
    "        taggedTxt.append(tggd)\n",
    "        \n",
    "    column = pd.Series(taggedTxt, index = df.index)\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_txt(txt, ps):\n",
    "    #is passed a string of raw text\n",
    "    \n",
    "    #this is just nltk's English Stop words with a couple things deleted, like \"didn't\", \"couldn't\" and their variations    \n",
    "    custStopWords = ['to', 'further', 'ma', 'a', 'no',  'or', 'ours', 'once', 'before', 'out', \"doesn't\", \n",
    "                      've', 'm', \"you've\", \"needn't\", 'you', 'not', 'so', 'off', 'under', 'most', 'which', \n",
    "                      'more', 'ourselves', 'about', 'down', 'isn', 'they', 'his', \"she's\", 'only', 'how', \n",
    "                      'had', 'again', 'by', 'after', 'shan', 'their', 'some', \"hasn't\", 'mustn', 'yours', \n",
    "                      'is', 'who', 'we', 'because', \"you'll\", 'it', 'has', 'both', 'here', \"don't\", 'than', \n",
    "                      'through', 'any', 'did', 'its', 'own', 'being', 'all', 'yourself', 'needn', 'd', 'o', \n",
    "                      \"weren't\",  'itself', 'what', 're', 'my', 'there', 'ain', 'i', \"isn't\", \"aren't\", 'if', \n",
    "                      'll', 'wasn', 'of', 'your', 'an',  'over', 'wouldn', 'y', \"mightn't\", 'between', 'mightn',\n",
    "                      \"hadn't\", 's', 'on', 'while', 'from', 'have', \"shan't\", 'then', \"mustn't\", 'will', 'below',\n",
    "                      'where', 'been', 'same', 'don', 'myself', 'until', 'other', 'doesn', 'but', 'above', 'can', \n",
    "                      'for', 'and', 'against', \"you'd\", 'him', 'does', 'into', 'are', 'these', 'few', 'himself', \n",
    "                      'aren', \"wasn't\", 'at', 'too', \"should've\", 'should', 'those', \"that'll\", 'me', 'hasn', 'shouldn',\n",
    "                      'themselves', 'weren', 'our', 'as', 'be', \"it's\", 'the', 'was', 'up', 'hadn', 'am',\n",
    "                      'this', 'yourselves', 'that', \"you're\", 'having', 'each', 'do', 'she', 'them', 'very',\n",
    "                      'nor', 'he', 'whom', 'now', 'won', 'during', 'her', 'hers', 'were', 'just', 'with', \n",
    "                      'why', \"wouldn't\", 'when', 'herself', \"won't\", \"shouldn't\", 'such', 'doing', 'in', 'theirs']\n",
    "    \n",
    "    trashPOStags = ['NNP', 'NNPS', 'PRP', 'PRP$', 'WP$', 'WP', 'WDT']\n",
    "\n",
    "    taggedLst= []\n",
    "    \n",
    "\n",
    "    toked = PunktSentenceTokenizer().tokenize(txt)  #tokenized into sentences\n",
    "    \n",
    "    for s in toked: \n",
    "        \n",
    "        words = word_tokenize(s)  #tokenize into words\n",
    "        \n",
    "        words = [words[i] for i in range(len(words)) if words[i] not in string.punctuation]\n",
    "        \n",
    "        words = nltk.pos_tag(words) #creates a list of (word, posTag) pairs\n",
    "        \n",
    "        words = [(ps.stem(word[0]), word[1]) for word in words if word[0] not in custStopWords] #stem the words.\n",
    "\n",
    "        for word in words:  #word -> (word, pos)\n",
    "            if not word[1] in trashPOStags: #if not a proper noun, etc\n",
    "                taggedLst.append(word)\n",
    "    \n",
    "\n",
    "    return taggedLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeats(series):\n",
    "    \n",
    "    valList = []\n",
    "    \n",
    "    \n",
    "    for i in series.index:\n",
    "        sample = series.iloc[i]\n",
    "        stemTxt = \"\"\n",
    "        for w in sample:\n",
    "            stemTxt += str(w[0])\n",
    "            stemTxt += \" \"\n",
    "        valList.append(stemTxt)\n",
    "    stemTxtCol = pd.Series(valList)\n",
    "    \n",
    "    return stemTxtCol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reviews = getData()\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews.loc[:, \"pos_stem\"] = addPOS(reviews.loc[:, 'text'], ps)\n",
    "#addPOS() takes a Series, and returns a Series containing processed, tagged words as a new column of the df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews.loc[:, \"stemTxtCol\"] = getFeats(reviews.loc[:, 'pos_stem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txtList = [reviews.loc[:, \"stemTxtCol\"].iloc[i] \n",
    "           for i in reviews.loc[:, \"stemTxtCol\"].index]\n",
    "\n",
    "piv = (len(txtList) // 4) * 3\n",
    "\n",
    "trainTxt = txtList[:piv]\n",
    "testTxt = txtList[piv:]\n",
    "\n",
    "targArr = np.zeros(len(txtList))\n",
    "\n",
    "for i in range(len(txtList)):\n",
    "    if reviews.iloc[i].loc[\"label\"] == \"i\":\n",
    "        targArr[i] = 1\n",
    "        \n",
    "        \n",
    "trTarg = targArr[:piv]\n",
    "tsTarg = targArr[piv:]\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees, SGD, Multinomial Naive Bayes\n",
    "\n",
    "## Results have been promising  ^.^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   36.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score from randSearch:\n",
      "0.782997762864\n",
      "\n",
      "Best params:\n",
      "{'vect__ngram_range': (1, 2), 'clf__min_samples_leaf': 3, 'clf__max_features': None, 'clf__max_depth': 2}\n",
      "\n",
      "classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.85      0.99      0.91       112\n",
      "        1.0       0.95      0.47      0.63        38\n",
      "\n",
      "avg / total       0.87      0.86      0.84       150\n",
      "\n",
      "\n",
      "confusion mtx\n",
      "[[111   1]\n",
      " [ 20  18]]\n"
     ]
    }
   ],
   "source": [
    "dtParams =  {'vect__ngram_range': [(1, i+2) for i in range(4)],\n",
    "            'clf__max_depth': [i+2 for i in range(15)]+[None],\n",
    "            \"clf__min_samples_leaf\": [(i+1) for i in range(10)], \n",
    "            \"clf__max_features\": [(i+5) for i in range(10)]+[None]}\n",
    "\n",
    "\n",
    "\n",
    "dt_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', \n",
    "                      DecisionTreeClassifier())])\n",
    "\n",
    "\n",
    "randSearch = RandomizedSearchCV(estimator = dt_clf, \n",
    "                   param_distributions = dtParams,\n",
    "                   n_iter = 100, cv = 3, verbose=2,\n",
    "                   random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "randSearch.fit(trainTxt, trTarg)\n",
    "\n",
    "best_random = randSearch.best_estimator_\n",
    "\n",
    "_ = best_random.fit(trainTxt, trTarg)\n",
    "\n",
    "predicted = best_random.predict(testTxt)\n",
    "\n",
    "\n",
    "print(\"Best score from randSearch:\")\n",
    "print(randSearch.best_score_)\n",
    "print()\n",
    "print(\"Best params:\")\n",
    "print(randSearch.best_params_)\n",
    "print()\n",
    "\n",
    "print(\"classification report\")\n",
    "print(metrics.classification_report(tsTarg, predicted))\n",
    "print()\n",
    "print(\"confusion mtx\")\n",
    "print(metrics.confusion_matrix(tsTarg, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aubrey\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   16.1s\n"
     ]
    }
   ],
   "source": [
    "sgdParams = {'vect__max_features': [i+5 for i in range(15)]+[None],\n",
    "             'vect__ngram_range': [(1, i+2) for i in range(4)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3)}\n",
    "sgd_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', \n",
    "                      SGDClassifier())])\n",
    "\n",
    "randSearch = RandomizedSearchCV(estimator = sgd_clf, \n",
    "                   param_distributions = sgdParams,\n",
    "                   n_iter = 100, cv = 3, verbose=2,\n",
    "                   random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "randSearch.fit(trainTxt, trTarg)\n",
    "\n",
    "best_random = randSearch.best_estimator_\n",
    "\n",
    "_ = best_random.fit(trainTxt, trTarg)\n",
    "\n",
    "predicted = best_random.predict(testTxt)\n",
    "print(\"Best score from randSearch:\")\n",
    "print(randSearch.best_score_)\n",
    "print()\n",
    "print(\"Best params:\")\n",
    "print(randSearch.best_params_)\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"classification report\")\n",
    "print(metrics.classification_report(tsTarg, predicted))\n",
    "print()\n",
    "print(\"confusion mtx\")\n",
    "print(metrics.confusion_matrix(tsTarg, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnbParams = {\"vect__ngram_range\": [(1, 1), (1, 2), (1,3), (1,4)],\n",
    "            'vect__max_features': [i+15 for i in range(10)],\n",
    "            \"tfidf__use_idf\": (True, False),\n",
    "            \"clf__alpha\": np.arange(0, 1.1, 0.1)}\n",
    "\n",
    "mnb_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "randSearch = RandomizedSearchCV(estimator = mnb_clf, \n",
    "                   param_distributions = mnbParams,\n",
    "                   n_iter = 100, cv = 3, verbose=2,\n",
    "                   random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "randSearch.fit(trainTxt, trTarg)\n",
    "\n",
    "best_random = randSearch.best_estimator_\n",
    "\n",
    "_ = best_random.fit(trainTxt, trTarg)\n",
    "\n",
    "predicted = best_random.predict(testTxt)\n",
    "\n",
    "\n",
    "print(randSearch.best_score_)\n",
    "print(randSearch.best_params_)\n",
    "print()\n",
    "print(\"classification report\")\n",
    "print(metrics.classification_report(tsTarg, predicted))\n",
    "print()\n",
    "print(\"confusion mtx\")\n",
    "print(metrics.confusion_matrix(tsTarg, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randofm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rf_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())])\n",
    "\n",
    "randSearch = RandomizedSearchCV(estimator = rf_clf, \n",
    "                   param_distributions = rfParams,\n",
    "                   n_iter = 100, cv = 3, verbose=2,\n",
    "                   random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "randSearch.fit(trainTxt, trTarg)\n",
    "\n",
    "best_random = randSearch.best_estimator_\n",
    "\n",
    "_ = best_random.fit(trainTxt, trTarg)\n",
    "\n",
    "predicted = best_random.predict(testTxt)\n",
    "\n",
    "\n",
    "print(\"Best score from randSearch:\")\n",
    "print(randSearch.best_score_)\n",
    "print()\n",
    "print(\"Best params:\")\n",
    "print(randSearch.best_params_)\n",
    "print()\n",
    "\n",
    "print(\"classification report\")\n",
    "print(metrics.classification_report(tsTarg, predicted))\n",
    "print()\n",
    "print(\"confusion mtx\")\n",
    "print(metrics.confusion_matrix(tsTarg, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C-SVM Classification\n",
    "\n",
    "## This ones weird. \n",
    "\n",
    "....yeah so its probably overfitted because of C. It might be worth keeping around and playing with the parameters. ¯\\\\__(ツ)__/¯  I grabbed this from [statsoft](url_hehttp://www.statsoft.com/textbook/support-vector-machinesre):\n",
    "\n",
    "<img src = \"csvm.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686750085121\n",
      "{'clf__degree': 1, 'tfidf__use_idf': True, 'vect__max_features': 18, 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "\n",
      "0.733333333333\n",
      "\n",
      "classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      1.00      0.83        99\n",
      "        1.0       1.00      0.22      0.35        51\n",
      "\n",
      "avg / total       0.81      0.73      0.67       150\n",
      "\n",
      "\n",
      "confusion mtx\n",
      "[[99  0]\n",
      " [40 11]]\n"
     ]
    }
   ],
   "source": [
    "svcParams = {\"vect__ngram_range\": [(1, 1), (1, 2), (1,3), (1,4)],\n",
    "            'vect__max_features': [i+15 for i in range(10)],\n",
    "            \"tfidf__use_idf\": (True, False), \n",
    "            \"clf__degree\": [i+1 for i in range(4)]}\n",
    "\n",
    "svc_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC())])\n",
    "\n",
    "\n",
    "svc_clf = GridSearchCV(svc_clf, svcParams,\n",
    "                      cv=5, iid=False, n_jobs=-1)\n",
    "svc_clf = svc_clf.fit(trainTxt, trTarg)\n",
    "print(svc_clf.best_score_)\n",
    "print(svc_clf.best_params_) \n",
    "\n",
    "best = svc_clf.best_estimator_\n",
    "_ = best.fit(trainTxt, trTarg)\n",
    "\n",
    "predicted = best.predict(testTxt)\n",
    "print()\n",
    "print()\n",
    "print(np.mean(predicted == tsTarg))\n",
    "print()\n",
    "print(\"classification report\")\n",
    "print(metrics.classification_report(tsTarg, predicted))\n",
    "print()\n",
    "print(\"confusion mtx\")\n",
    "print(metrics.confusion_matrix(tsTarg, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

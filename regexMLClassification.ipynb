{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex-ML Hybrid Classifier Model\n",
    "\n",
    "\n",
    "### An attempt at combining regex searches for targetted patterns with ML models\n",
    "\n",
    "Starting with a simple majority vote idea -- votes from DT, SGD, and regex classifiers. \n",
    "ML models pushed through pipelines to get best params, then test set is classified with best estimator given by randomizedSearchCV.\n",
    "As of now, each classifiers vote is weighted equally. Would like to tweak this, as well as give more weight to specific regex patterns over others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aubrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import PunktSentenceTokenizer, sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy \n",
    "import sklearn\n",
    "import random\n",
    "from pprint import pprint\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    \n",
    "    data = pd.read_csv(\"BookReviews.csv\", encoding = 'latin1')\n",
    "    \n",
    "    data = data.loc[:, ~data.columns.str.contains('^Unnamed')]  #had several (about 4) columns titles \"Unnamed\"\n",
    "    transltr = str.maketrans('', '', string.punctuation)\n",
    "    for i in range(data['text'].size):\n",
    "        data['text'].iloc[i] = data['text'].iloc[i].lower()\n",
    "        data['text'].iloc[i] = data['text'].iloc[i].replace(\"%\", \"PERCENT\")\n",
    "        data['text'].iloc[i] = data['text'].iloc[i].translate(transltr)\n",
    "        if data['label'].iloc[i] == \"i\":\n",
    "            data['label'].iloc[i] = 1\n",
    "        else: \n",
    "            data['label'].iloc[i] = 0\n",
    "            \n",
    "    data = data.sample(frac = 1).reset_index(drop = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partitionData(df):\n",
    "    \n",
    "    txtSeries = df.loc[:,\"cleanTxt\"]\n",
    "    labelSeries = df.loc[:,\"label\"]\n",
    "    \n",
    "    txtList = [txtSeries.iloc[i] for i in range(txtSeries.size)]\n",
    "    labelList = [labelSeries.iloc[i] for i in range(labelSeries.size)]\n",
    "           \n",
    "    piv = (len(txtSeries) // 3) * 2\n",
    "\n",
    "    trainTxt = txtList[:piv]\n",
    "    testTxt = txtList[piv:]\n",
    "\n",
    "    trTarg = labelList[:piv]\n",
    "    tsTarg = labelList[piv:]\n",
    "    \n",
    "    return [[trainTxt, trTarg],[testTxt, tsTarg]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regexCLF(txt):\n",
    "    reg1 = r'dnf'\n",
    "    reg2 = r'g[ai]ve up'\n",
    "    reg3 = r'(finish|listen|read|dnf)[\\w\\s]*(chapter|p(age|g)|PERCENT|half ?(way)?)'\n",
    "    reg4 = r'(have|could|did|never|can(not|t))[\\w\\s]*(finish|g[eo]t (into|through)|go on)'\n",
    "\n",
    "\n",
    "    reg = reg1 + r'|' + reg2 + r'|'+ reg3 + r'|'+ reg4\n",
    "\n",
    "    match = []\n",
    "    \n",
    "    for sample in txt:\n",
    "        matchBool = 0\n",
    "        if re.findall(reg, sample):\n",
    "            matchBool = 1\n",
    "        \n",
    "        match.append(matchBool)\n",
    "        \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTxt(df, ps):\n",
    "    \n",
    "    cleanTxt = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        \n",
    "        txt = str(df.iloc[i])  #I had problems where this was a float, which doesn't make much sense\n",
    "        tggd = tag_txt(txt, ps)\n",
    "        stemTxt = \"\"\n",
    "        for w in tggd:\n",
    "            stemTxt += str(w[0])\n",
    "            stemTxt += \" \"\n",
    "        cleanTxt.append(stemTxt)\n",
    "    \n",
    "    column = pd.Series(cleanTxt)\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_txt(txt, ps):\n",
    "    #is passed a string of raw text\n",
    "    \n",
    "    #this is just nltk's English Stop words with a couple things deleted, like \"didn't\", \"couldn't\" and their variations    \n",
    "    custStopWords = ['to', 'further', 'ma', 'a', 'no',  'or', 'ours', 'once', 'before', 'out', \"doesn't\", \n",
    "                      've', 'm', \"you've\", \"needn't\", 'you', 'not', 'so', 'off', 'under', 'most', 'which', \n",
    "                      'more', 'ourselves', 'about', 'down', 'isn', 'they', 'his', \"she's\", 'only', 'how', \n",
    "                      'had', 'again', 'by', 'after', 'shan', 'their', 'some', \"hasn't\", 'mustn', 'yours', \n",
    "                      'is', 'who', 'we', 'because', \"you'll\", 'it', 'has', 'both', 'here', \"don't\", 'than', \n",
    "                      'through', 'any', 'did', 'its', 'own', 'being', 'all', 'yourself', 'needn', 'd', 'o', \n",
    "                      \"weren't\",  'itself', 'what', 're', 'my', 'there', 'ain', 'i', \"isn't\", \"aren't\", 'if', \n",
    "                      'll', 'wasn', 'of', 'your', 'an',  'over', 'wouldn', 'y', \"mightn't\", 'between', 'mightn',\n",
    "                      \"hadn't\", 's', 'on', 'while', 'from', 'have', \"shan't\", 'then', \"mustn't\", 'will', 'below',\n",
    "                      'where', 'been', 'same', 'don', 'myself', 'until', 'other', 'doesn', 'but', 'above', 'can', \n",
    "                      'for', 'and', 'against', \"you'd\", 'him', 'does', 'into', 'are', 'these', 'few', 'himself', \n",
    "                      'aren', \"wasn't\", 'at', 'too', \"should've\", 'should', 'those', \"that'll\", 'me', 'hasn', 'shouldn',\n",
    "                      'themselves', 'weren', 'our', 'as', 'be', \"it's\", 'the', 'was', 'up', 'hadn', 'am',\n",
    "                      'this', 'yourselves', 'that', \"you're\", 'having', 'each', 'do', 'she', 'them', 'very',\n",
    "                      'nor', 'he', 'whom', 'now', 'won', 'during', 'her', 'hers', 'were', 'just', 'with', \n",
    "                      'why', \"wouldn't\", 'when', 'herself', \"won't\", \"shouldn't\", 'such', 'doing', 'in', 'theirs']\n",
    "    \n",
    "    trashPOStags = ['NNP', 'NNPS', 'PRP', 'PRP$', 'WP$', 'WP', 'WDT']\n",
    "\n",
    "    taggedLst= []\n",
    "    toked = PunktSentenceTokenizer().tokenize(txt)  #tokenized into sentences\n",
    "    \n",
    "    for s in toked: \n",
    "        \n",
    "        words = word_tokenize(s)  #tokenize into words\n",
    "        \n",
    "        words = [words[i] for i in range(len(words)) if words[i] not in string.punctuation]\n",
    "        \n",
    "        words = nltk.pos_tag(words) #creates a list of (word, posTag) pairs\n",
    "        \n",
    "        words = [(ps.stem(word[0]), word[1]) for word in words if word[0] not in custStopWords] #stem the words.\n",
    "\n",
    "        for word in words:  #word -> (word, pos)\n",
    "            if  word[1] not in trashPOStags: #if not a proper noun, etc\n",
    "                taggedLst.append(word)\n",
    "    \n",
    "\n",
    "    return taggedLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDTclf(trTxt, trTrgs):\n",
    "\n",
    "    dtParams =  {'vect__ngram_range': [(1, i+2) for i in range(4)],\n",
    "                'clf__max_depth': [i+2 for i in range(10)],\n",
    "                \"clf__min_samples_leaf\": [(i+1) for i in range(10)],\n",
    "                \"clf__max_features\": [(i+5) for i in range(100)]}\n",
    "\n",
    "    dt_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', \n",
    "                          DecisionTreeClassifier())])\n",
    "\n",
    "\n",
    "    randSearch = RandomizedSearchCV(estimator = dt_clf, \n",
    "                       param_distributions = dtParams,\n",
    "                       n_iter = 100, cv = 3, verbose=2,\n",
    "                       random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    randSearch.fit(trTxt, trTrgs)\n",
    "\n",
    "    best_random = randSearch.best_estimator_\n",
    "\n",
    "    best = best_random.fit(trTxt, trTrgs)\n",
    "\n",
    "    \n",
    "    print(\"     DT Training Summary\")\n",
    "    print(\"******************************\")\n",
    "    print(\"Best score from randSearch:\")\n",
    "    print(randSearch.best_score_)\n",
    "    print()\n",
    "    print(\"Best params:\")\n",
    "    print(randSearch.best_params_)\n",
    "    print()\n",
    "    \n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getSGDclf(trTxt, trTrgs):\n",
    "    sgdParams = {'vect__max_features': [i+5 for i in range(15)]+[None],\n",
    "                 'vect__ngram_range': [(1, i+2) for i in range(4)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf__alpha': (1e-2, 1e-3),\n",
    "                \"clf__loss\": [\"hinge\", \"log\", \"squared_hinge\"],\n",
    "                \"clf__penalty\": [\"none\", \"elasticnet\"]}\n",
    "    sgd_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier())])\n",
    "\n",
    "    randSearch = RandomizedSearchCV(estimator = sgd_clf, \n",
    "                       param_distributions = sgdParams,\n",
    "                       n_iter = 100, cv = 3, verbose=2,\n",
    "                       random_state=42, n_jobs = -1)\n",
    "    # Fit the random search model\n",
    "    randSearch.fit(trTxt, trTrgs)\n",
    "\n",
    "    best_random = randSearch.best_estimator_\n",
    "\n",
    "    best = best_random.fit(trTxt, trTrgs)\n",
    "    \n",
    "    print(\"   SGD Training Summary\")\n",
    "    print(\"**************************\")\n",
    "    print(\"Best score from randSearch:\")\n",
    "    print(randSearch.best_score_)\n",
    "    print()\n",
    "    print(\"Best params:\")\n",
    "    print(randSearch.best_params_)\n",
    "    print()\n",
    "    \n",
    "    return best\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def majorityClassify(dt, sgd, re):\n",
    "    \n",
    "    votes = []\n",
    "    \n",
    "    for i in range(len(dt)):\n",
    "        total = dt[i] + sgd[i] + re[i]\n",
    "        votes.append(total)\n",
    "        \n",
    "    for n in range(len(votes)):\n",
    "        if votes[n] > 1:\n",
    "            votes[n] = 1\n",
    "        else: votes[n] = 0\n",
    "    return votes\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metrics(trueLabel, predict):\n",
    "    \n",
    "    tpi = 0\n",
    "    fpi = 0\n",
    "    fni = 0\n",
    "    \n",
    "    tpc = 0\n",
    "    fpc = 0\n",
    "    fnc = 0\n",
    "\n",
    "\n",
    "    for i in range(len(trueLabel)):\n",
    "        if predict[i] == 1:\n",
    "            if trueLabel[i] == 1:\n",
    "                tpi += 1\n",
    "            else: fpi += 1 #fni\n",
    "        else:\n",
    "            if trueLabel[i] == 1:\n",
    "                fni += 1 #fpc\n",
    "            else:\n",
    "                tpc += 1\n",
    "\n",
    "    pi = tpi/(tpi + fpi)\n",
    "    ri = tpi/(tpi + fni)\n",
    "    f1i = 2*(pi*ri)/(pi+ri)\n",
    "    \n",
    "    pc = tpc/(tpc + fni)\n",
    "    rc = tpi/(tpc + fpi)\n",
    "    f1c = 2*(pc*rc)/(pc+rc)\n",
    "\n",
    "    print(\"   For incomplete reviews:\")\n",
    "    print(\"*******************************\")\n",
    "    print()\n",
    "    print(\"precision\")\n",
    "    print(pi)\n",
    "    print(\"recall\")\n",
    "    print(ri)\n",
    "    print(\"f1\")\n",
    "    print(f1i)\n",
    "    print()\n",
    "    print()\n",
    "    print(\"   For complete reviews:\")\n",
    "    print(\"*******************************\")\n",
    "    print()\n",
    "    print(\"precision\")\n",
    "    print(pc)\n",
    "    print(\"recall\")\n",
    "    print(rc)\n",
    "    print(\"f1\")\n",
    "    print(f1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    reviews = getData()\n",
    "    ps = PorterStemmer()\n",
    "    reviews.loc[:, \"cleanTxt\"] = cleanTxt(reviews.loc[:, 'text'], ps)\n",
    "    trn, tst = partitionData(reviews)\n",
    "    \n",
    "    dtCLF = getDTclf(trn[0], trn[1])\n",
    "    sgdCLF = getSGDclf(trn[0], trn[1])\n",
    "    \n",
    "    dtVotes = dtCLF.predict(tst[0])\n",
    "    sgdVotes = sgdCLF.predict(tst[0])\n",
    "    regexVotes = regexCLF(tst[0])\n",
    "    \n",
    "    votes = majorityClassify(dtVotes, sgdVotes, regexVotes)\n",
    "    \n",
    "    metrics(tst[1], votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   30.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     DT Training Summary\n",
      "******************************\n",
      "Best score from randSearch:\n",
      "0.688442211055\n",
      "\n",
      "Best params:\n",
      "{'vect__ngram_range': (1, 2), 'clf__min_samples_leaf': 10, 'clf__max_features': 93, 'clf__max_depth': 2}\n",
      "\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aubrey\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   27.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SGD Training Summary\n",
      "**************************\n",
      "Best score from randSearch:\n",
      "0.839195979899\n",
      "\n",
      "Best params:\n",
      "{'vect__ngram_range': (1, 4), 'vect__max_features': None, 'tfidf__use_idf': False, 'clf__penalty': 'elasticnet', 'clf__loss': 'hinge', 'clf__alpha': 0.001}\n",
      "\n",
      "   For incomplete reviews:\n",
      "*******************************\n",
      "\n",
      "precision\n",
      "0.8928571428571429\n",
      "recall\n",
      "0.373134328358209\n",
      "f1\n",
      "0.5263157894736842\n",
      "\n",
      "\n",
      "   For complete reviews:\n",
      "*******************************\n",
      "\n",
      "precision\n",
      "0.7543859649122807\n",
      "recall\n",
      "0.1893939393939394\n",
      "f1\n",
      "0.3027742571468808\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
